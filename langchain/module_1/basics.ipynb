{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "ef597741-3211-4ecc-92f7-f58023ee237e",
      "metadata": {
        "id": "ef597741-3211-4ecc-92f7-f58023ee237e"
      },
      "source": [
        "# LangChain Academy\n",
        "\n",
        "Welcome to LangChain Academy!\n",
        "\n",
        "## Context\n",
        "\n",
        "At LangChain, we aim to make it easy to build LLM applications. One type of LLM application you can build is an agent. There’s a lot of excitement around building agents because they can automate a wide range of tasks that were previously impossible.\n",
        "\n",
        "In practice though, it is incredibly difficult to build systems that reliably execute on these tasks. As we’ve worked with our users to put agents into production, we’ve learned that more control is often necessary. You might need an agent to always call a specific tool first or use different prompts based on its state.\n",
        "\n",
        "To tackle this problem, we’ve built [LangGraph](https://langchain-ai.github.io/langgraph/) — a framework for building agent and multi-agent applications. Separate from the LangChain package, LangGraph’s core design philosophy is to help developers add better precision and control into agent workflows, suitable for the complexity of real-world systems.\n",
        "\n",
        "## Course Structure\n",
        "\n",
        "The course is structured as a set of modules, with each module focused on a particular theme related to LangGraph. You will see a folder for each module, which contains a series of notebooks. A video will accompany each notebook to help walk through the concepts, but the notebooks are also stand-alone, meaning that they contain explanations and can be viewed independent of the videos. Each module folder also contains a `studio` folder, which contains a set of graphs that can be loaded into [LangGraph Studio](https://github.com/langchain-ai/langgraph-studio), our IDE for building LangGraph applications.\n",
        "\n",
        "## Setup\n",
        "\n",
        "Before you begin, please follow the instructions in the `README` to create an environment and install dependencies.\n",
        "\n",
        "## Chat models\n",
        "\n",
        "In this course, we'll be using [Chat Models](https://python.langchain.com/v0.2/docs/concepts/#chat-models), which do a few things take a sequence of messages as inputs and return chat messages as outputs. LangChain does not host any Chat Models, rather we rely on third party integrations. [Here](https://python.langchain.com/v0.2/docs/integrations/chat/) is a list of 3rd party chat model integrations within LangChain! By default, the course with use [GEMINI_API_KEY](https://ai.google.dev/gemini-api/docs/api-key/) because it is both popular and performant. As noted, please ensure that you have an `GEMINI_API_KEY`.\n",
        "\n",
        "Let's check that your `GEMINI_API_KEY` is set and, if not, you will be asked to enter it."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `%%capture` magic command in Jupyter notebooks is used to capture the output of a cell and suppress the display of output in the notebook. It can be especially useful when you want to suppress standard output (stdout) and error messages (stderr) from being displayed.\n",
        "\n",
        "### **Explanation of the components**:\n",
        "- `%%capture`: This is a cell magic command. It captures the output of all code in the cell, including standard output and standard error, and stores it in a variable or simply suppresses it.\n",
        "- `--no-stderr`: This is an optional flag that tells Jupyter to capture only the standard output (stdout) and **not** the standard error (stderr). If there are errors in the code, they won't be captured or suppressed, and they will still be shown in the output.\n",
        "\n",
        "### **Usage Example**:\n",
        "\n",
        "```python\n",
        "%%capture --no-stderr\n",
        "print(\"This is a normal output.\")\n",
        "raise ValueError(\"This is an error message.\")\n",
        "```\n",
        "\n",
        "In this example:\n",
        "- The **normal output** (`print(\"This is a normal output.\")`) would be captured and suppressed.\n",
        "- The **error message** (`ValueError(\"This is an error message.\")`) would still be shown because the `--no-stderr` flag is used, which ensures errors are not suppressed.\n",
        "\n",
        "### **What happens with the captured output?**\n",
        "If you want to access the captured output later, you can store it in a variable:\n",
        "\n",
        "```python\n",
        "%%capture captured_output\n",
        "print(\"This will be captured.\")\n",
        "```\n",
        "\n",
        "You can then retrieve and use the captured output:\n",
        "\n",
        "```python\n",
        "captured_output.show()  # This will display the captured output\n",
        "```\n",
        "\n",
        "### **Summary**:\n",
        "- `%%capture` suppresses output in Jupyter notebooks.\n",
        "- `--no-stderr` ensures that **only standard output (stdout)** is captured, and **standard error (stderr)** is shown.\n",
        "- It is useful when you want to prevent clutter in your notebook, especially for commands or code that produce unnecessary output."
      ],
      "metadata": {
        "id": "gImZnlolNSEI"
      },
      "id": "gImZnlolNSEI"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "0f9a52c8",
      "metadata": {
        "id": "0f9a52c8"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "!pip install -q langchain_google_genai langchain_core langchain_community tavily-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from google.colab import userdata\n",
        "API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(api_key=API_KEY,model='gemini-1.5-flash',temperature=0)\n",
        "llm.invoke(\"how are you ?\")"
      ],
      "metadata": {
        "id": "EfHd5vlIHKd8",
        "outputId": "5f010fa0-5b2c-412a-a0ec-01e5d0f08709",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "EfHd5vlIHKd8",
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I am doing well, thank you for asking!  As a large language model, I don't experience emotions or feelings in the same way humans do, but I'm functioning optimally and ready to assist you. How can I help you today?\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-40baf273-499b-4087-9c8d-8860cab9ba9a-0', usage_metadata={'input_tokens': 5, 'output_tokens': 52, 'total_tokens': 57, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "messages =[HumanMessage(content=\"hi\",name = \"arman\"),\n",
        "           AIMessage(content=\"you are helpful assistant of AI engineer\", name = \"AI Assistant\"),\n",
        "           HumanMessage(content=\"what is AI\", name = \"arman\")]\n",
        "llm.invoke(messages)\n"
      ],
      "metadata": {
        "id": "7LRBkQ3aIrwt",
        "outputId": "a6aa186d-54e3-4c68-b2f5-29ff9bae81fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "7LRBkQ3aIrwt",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Artificial intelligence (AI) is a broad field encompassing the development of computer systems capable of performing tasks that typically require human intelligence.  These tasks include things like:\\n\\n* **Learning:** Acquiring information and rules for using the information.\\n* **Reasoning:** Using rules to reach approximate or definite conclusions.\\n* **Problem-solving:** Finding solutions to complex situations.\\n* **Perception:** Interpreting sensory information like images, sound, and text.\\n* **Language understanding:** Processing and understanding human language.\\n\\nAI systems achieve these capabilities through various techniques, including:\\n\\n* **Machine learning (ML):** Algorithms that allow systems to learn from data without explicit programming.  This includes subfields like deep learning (using artificial neural networks with many layers) and reinforcement learning (learning through trial and error).\\n* **Natural language processing (NLP):** Enabling computers to understand, interpret, and generate human language.\\n* **Computer vision:** Enabling computers to \"see\" and interpret images and videos.\\n* **Robotics:** Combining AI with physical robots to perform tasks in the real world.\\n\\nIt\\'s important to note that AI is not a single technology but rather a collection of techniques and approaches.  The term often evokes images of sentient robots, but most current AI systems are focused on specific tasks and lack general intelligence or consciousness.  The field is constantly evolving, with new breakthroughs and applications emerging regularly.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-fcee8765-9c22-4cbc-abea-204718168a56-0', usage_metadata={'input_tokens': 14, 'output_tokens': 287, 'total_tokens': 301, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Streaming true\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "messages =[HumanMessage(content=\"hi\",name = \"arman\"),\n",
        "           AIMessage(content=\"you are helpful assistant of AI engineer\", name = \"AI Assistant\"),\n",
        "           HumanMessage(content=\"what is AI\", name = \"arman\")]\n",
        "for chunk in llm.stream(messages):\n",
        "  print(chunk)"
      ],
      "metadata": {
        "id": "ajydDGgYMELv",
        "outputId": "139a7b07-e9df-41d2-a361-03e68941a3f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "ajydDGgYMELv",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Artificial' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-b0038977-240f-43ec-b6e5-1867424bd271' usage_metadata={'input_tokens': 14, 'output_tokens': 0, 'total_tokens': 14, 'input_token_details': {'cache_read': 0}}\n",
            "content=' intelligence (AI) is a broad field encompassing the development of computer systems capable of' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-b0038977-240f-43ec-b6e5-1867424bd271' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=' performing tasks that typically require human intelligence.  These tasks include things like:\\n\\n' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-b0038977-240f-43ec-b6e5-1867424bd271' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content='* **Learning:** Acquiring information and rules for using the information.\\n* **Reasoning:** Using rules to reach approximate or definite conclusions.\\n* **' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-b0038977-240f-43ec-b6e5-1867424bd271' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content='Problem-solving:** Finding solutions to complex situations.\\n* **Perception:** Interpreting sensory information like images, sound, and text.\\n* **Language understanding:**' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-b0038977-240f-43ec-b6e5-1867424bd271' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=' Processing and understanding human language.\\n\\nAI systems achieve these capabilities through various techniques, including:\\n\\n* **Machine learning (ML):** Algorithms that allow systems to learn from data without explicit programming.  This includes subfields like deep learning (' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-b0038977-240f-43ec-b6e5-1867424bd271' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content='using artificial neural networks with many layers) and reinforcement learning (learning through trial and error).\\n* **Natural language processing (NLP):** Enabling computers to understand, interpret, and generate human language.\\n* **Computer vision:** Enabling computers' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-b0038977-240f-43ec-b6e5-1867424bd271' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=' to \"see\" and interpret images and videos.\\n* **Robotics:** Combining AI with physical robots to perform tasks in the real world.\\n\\nIt\\'s important to note that AI is not a single technology but rather a collection of techniques and approaches.  The term often evokes images of sentient robots, but most current' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-b0038977-240f-43ec-b6e5-1867424bd271' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=' AI systems are focused on specific tasks and lack general intelligence or consciousness.  The field is constantly evolving, with new breakthroughs and applications emerging regularly.\\n' additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'safety_ratings': []} id='run-b0038977-240f-43ec-b6e5-1867424bd271' usage_metadata={'input_tokens': 0, 'output_tokens': 287, 'total_tokens': 287, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "from google.colab import userdata\n",
        "GEMINI_API_KEY = userdata.get('GEMINI_API_KEY')\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    api_key=GEMINI_API_KEY,\n",
        "    temperature=0  # temperature 0 means it give you more save and predicative response but if we set the temperature 0.7 to 1 it means it is more create it give me unexpected response\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ch7HciDZXh3F"
      },
      "id": "Ch7HciDZXh3F",
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result  = llm.invoke(\"Who won 2024 US presidential ellection?\")\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKnOnfj3YZYT",
        "outputId": "f85f98a2-83c0-4c48-ade8-0e32da40c894"
      },
      "id": "KKnOnfj3YZYT",
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The 2024 US presidential election has not yet happened.  The election will be held in November 2024.\\n', additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-348bdc7c-a03f-4d9a-90db-0a2c4646f6dd-0', usage_metadata={'input_tokens': 13, 'output_tokens': 29, 'total_tokens': 42, 'input_token_details': {'cache_read': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28450d1b",
      "metadata": {
        "id": "28450d1b"
      },
      "source": [
        "Chat models in LangChain have a number of [default methods](https://python.langchain.com/v0.2/docs/concepts/#runnable-interface). For the most part, we'll be using:\n",
        "\n",
        "* `stream`: stream back chunks of the response\n",
        "* `invoke`: call the chain on an input\n",
        "\n",
        "And, as mentioned, chat models take [messages](https://python.langchain.com/v0.2/docs/concepts/#messages) as input. Messages have a role (that describes who is saying the message) and a content property. We'll be talking a lot more about this later, but here let's just show the basics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b1280e1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1280e1b",
        "outputId": "ad53b729-941a-4da4-ac3b-d1a683c8661f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='There' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 39, 'output_tokens': 0, 'total_tokens': 39, 'input_token_details': {'cache_read': 0}}\n",
            "content=' are several ways to learn LangChain, depending on your learning style and prior experience' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=':\\n\\n**1. Official Documentation:** The best place to start is the official' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=' LangChain documentation.  It\\'s well-structured and provides comprehensive tutorials and examples.  Look for their \"Getting Started\" guides and work your way through' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=' the examples.\\n\\n**2. Tutorials and Blog Posts:** Many tutorials and blog posts are available online that cover various aspects of LangChain. Search for \"Lang' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content='Chain tutorial\" or \"LangChain example\" on Google, YouTube, and Medium.  Look for tutorials that focus on specific use cases that interest you, such as building a chatbot or a question-answering system.\\n\\n**3.' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=' GitHub Repository:** Explore the LangChain GitHub repository.  You can find the source code, contribute to the project, and see how different components are implemented.  Reading the code can be a great way to deepen your understanding.\\n\\n**4' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content='. Example Projects:**  The LangChain documentation and GitHub repository contain many example projects.  Start by running and modifying these examples to understand how different parts of the framework work together.  Try to break them and see what happens – this is a great way to learn.\\n\\n**5. Community and Forums:** Engage with' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=\" the LangChain community.  Ask questions on forums like Stack Overflow or join the LangChain Discord server (if one exists).  This is a great way to get help and learn from others' experiences.\\n\\n**6. Courses (Future Possibility):** While there aren't many dedicated LangChain courses yet, as\" additional_kwargs={} response_metadata={'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=' the framework is relatively new, you might find relevant courses on broader topics like large language models (LLMs) or building AI applications.  These courses might include sections on LangChain or provide a foundation that makes learning LangChain easier.\\n\\n\\n**To get started quickly, I recommend:**\\n\\n* **Familiarize yourself with' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=' the basic concepts:** Understand what LLMs are and how they work.  Having a basic understanding of Python programming is also essential.\\n* **Follow the official \"Getting Started\" guide:** This will walk you through setting up LangChain and building a simple application.\\n* **Choose a specific project:**  Instead of' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=' trying to learn everything at once, focus on a particular application you want to build (e.g., a chatbot, a summarizer, a question-answering system).  This will give you a clear goal and make the learning process more engaging.\\n\\n\\nRemember to be patient and persistent.  LangChain is a' additional_kwargs={} response_metadata={'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 0, 'output_tokens': 0, 'total_tokens': 0, 'input_token_details': {'cache_read': 0}}\n",
            "content=\" powerful framework, but it takes time and practice to master.  Start with the basics, gradually increase the complexity of your projects, and don't hesitate to ask for help when you need it.\\n\" additional_kwargs={} response_metadata={'finish_reason': 'STOP', 'safety_ratings': []} id='run-a456fc4a-660c-4bb5-836e-279f6e90796e' usage_metadata={'input_tokens': 0, 'output_tokens': 554, 'total_tokens': 554, 'input_token_details': {'cache_read': 0}}\n"
          ]
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# Create a message\n",
        "# Message list\n",
        "#In the HumanMessage and AIMessage classes from LangChain, the name attribute is used to label the sender of the message.\n",
        "messages = [\n",
        "    HumanMessage(content=\"Hi\", name=\"Human Student\"),\n",
        "    AIMessage(content='Hi! How can I help you today? \\n', name=\"AI Assistant\"),\n",
        "    HumanMessage(content=\"What is LangChain?\", name=\"Human Student\"),\n",
        "    AIMessage(content='LangChain is a framework for developing applications powered by language models.', name=\"AI Assistant\"),\n",
        "    HumanMessage(content=\"How can I learn\", name=\"Human Student\"),\n",
        "    ]\n",
        "\n",
        "# Invoke the model with a list of messages\n",
        "llm.invoke(messages)\n",
        "for chunk in llm.stream(messages):\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cac73e4c",
      "metadata": {
        "id": "cac73e4c"
      },
      "source": [
        "We get an `AIMessage` response. Also, note that we can just invoke a chat model with a string. When a string is passed in as input, it is converted to a `HumanMessage` and then passed to the underlying model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "582c0e5a",
      "metadata": {
        "id": "582c0e5a"
      },
      "source": [
        "The interface is consistent across all chat models and models are typically initialized once at the start up each notebooks.\n",
        "\n",
        "So, you can easily switch between models without changing the downstream code if you have strong preference for another provider.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ad0069a",
      "metadata": {
        "id": "3ad0069a"
      },
      "source": [
        "## Search Tools\n",
        "\n",
        "You'll also see [Tavily](https://tavily.com/) in the README, which is a search engine optimized for LLMs and RAG, aimed at efficient, quick, and persistent search results. As mentioned, it's easy to sign up and offers a generous free tier. Some lessons (in Module 4) will use Tavily by default but, of course, other search tools can be used if you want to modify the code for yourself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "52d69da9",
      "metadata": {
        "id": "52d69da9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "TAVILY_API_KEY = userdata.get('TAVILY_API_KEY')\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "tavily_search = TavilySearchResults(max_results=1)\n",
        "search_docs = tavily_search.invoke(\"Who won 2024 US presidential ellection?\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "search_docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWzZ6zAvb2Fv",
        "outputId": "5d7a0e6d-e90f-4cf0-cd5f-82dd5af2a1b4"
      },
      "id": "JWzZ6zAvb2Fv",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'url': 'https://www.indystar.com/story/news/politics/elections/2024/11/06/2024-us-election-results-today-who-won-united-states-presidential-election-november-5-usa-elections/76076772007/',\n",
              "  'content': \"2024 United States presidential election results: Who won US President? Election Day 2024 has come and gone as the dust settles in the battle between Donald Trump\\xa0and\\xa0Kamala Harris\\xa0for President of the United States. Indiana was one of several states to back Trump as their candidate of choice in election results, who ultimately received the 270 electoral votes required Tuesday night to secure the presidency. 2024 US presidential election results: Who won President of the United States − Donald Trump or Kamala Harris? 2024 US presidential election live results map by state: Who won President of the United States − Donald Trump or Kamala Harris? Visit indystar.com/elections/results to see Tuesday's U.S. presidential voting results map by state.\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEMINI_API_KEY is loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv( \"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise valueError(\"GEMINI_API_KEY is not set in the .env file please write api key and proceed further\")\n",
    "print(\"GEMINI_API_KEY is loaded successfully\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings,ChatGoogleGenerativeAI \n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "faq_data = [\n",
    "    {'question': \"How do I track my order?\", 'answer': \"You can track your order by logging into your account and going to the 'Order History' section.\"},\n",
    "    {'question': \"What is your return policy?\", 'answer': \"Our return policy allows returns within 30 days of purchase.\"},\n",
    "    {\"question\": \"Can I cancel or modify my order after placing it?\",\"answer\": \"Yes, you can cancel or modify your order within 24 hours of placing it by contacting our customer support team.\"},\n",
    "    {\"question\": \"What should I do if I receive a damaged or incorrect item?\", \"answer\": \"If you receive a damaged or incorrect item, please contact our support team immediately with your order details and a photo of the item.\"},\n",
    "    {\"question\": \"Do you offer discounts or promotional codes?\",\"answer\": \"Yes, we regularly offer discounts and promotional codes. Sign up for our newsletter or follow us on social media to stay updated.\"},\n",
    "    {\"question\": \"How long does delivery take?\",  \"answer\": \"Delivery typically takes 3-7 business days for domestic orders and 7-14 business days for international orders.\"},\n",
    "    {\"question\": \"Is my personal information secure?\", \"answer\": \"Yes, we use industry-standard encryption and security measures to protect your personal information and ensure safe transactions.\"}\n",
    "]\n",
    "\n",
    "\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\" ,google_api_key=GEMINI_API_KEY)\n",
    "def faq_data_with_embeddings(faq_data):\n",
    "    for faq in faq_data:\n",
    "        faq['embedding'] = gemini_embeddings.embed_query(faq['question'])\n",
    "    return faq_data\n",
    "\n",
    "\n",
    "enriched_faq_data = faq_data_with_embeddings(faq_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_faq_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "import os   \n",
    "from langchain.prompts import PromptTemplate \n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings,ChatGoogleGenerativeAI \n",
    "from langchain_core.messages import HumanMessage, AIMessage \n",
    "import numpy as np\n",
    " \n",
    " \n",
    "\n",
    "MATCHING_THRESHOLD = 0.78\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model = \"models/embedding-001\",google_api_key = GEMINI_API_KEY)\n",
    "  \n",
    "    \n",
    " \n",
    "def find_similar(query_emb:list[int],question_emb:list[int]) ->int:\n",
    "    similar = np.dot(query_emb,question_emb) / (np.linalg.norm(query_emb) * np.linalg.norm(question_emb))   \n",
    "    return similar\n",
    "     \n",
    "def rag_system(query:str):\n",
    "    try:\n",
    "        if not query.strip():\n",
    "            raise HTTPException(status_code=400, detail=\"Query cannot be empty.\")\n",
    "     \n",
    "        try:\n",
    "            query_embedding = gemini_embeddings.embed_query(query)\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500, detail=\"An error occurred while embedding the query.\")\n",
    "        try:\n",
    "            for index,item in enumerate(enriched_faq_data):\n",
    "                find_similarity = find_similar(query_embedding,item[\"embedding\"])\n",
    "                enriched_faq_data[index]['similarity'] = find_similarity\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500,  detail=f\"Error during similarity calculation: {str(e)}\")    \n",
    "        try:\n",
    "            final_faq = [(faq[\"question\"], faq[\"answer\"]) for faq in enriched_faq_data if faq[\"similarity\"] >= MATCHING_THRESHOLD] \n",
    "            context = \"\\n\".join([f\"Question: {faq['question']}\\nAnswer: {faq['answer']}\" for faq in enriched_faq_data])\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500,  detail=f\"Error during formatting the final response: {str(e)}\")\n",
    "        \n",
    "        prompt_template = PromptTemplate(\n",
    "            input_variables=[\"context\", \"user_query\"],\n",
    "            template=\"\"\"\n",
    "                You are a helpful and knowledgeable chatbot for an e-commerce website. Your job is to answer customer questions based on the provided context. If the answer cannot be found in the context, respond politely with: \"I’m sorry, I don’t know the answer to that right now.\"\n",
    "\n",
    "                ### Context:  \n",
    "                {context}\n",
    "\n",
    "                ### User Question:  \n",
    "                {user_query}\n",
    "\n",
    "                ### Guidelines for Responses:\n",
    "                1. Answer based on the provided context.\n",
    "                2. Be concise, friendly, and professional.\n",
    "                3. If unsure, say: \"I’m sorry, I don’t know the answer to that right now.\"\n",
    "                4. Offer help where possible, like checking stock, suggesting products, or explaining policies.\n",
    "\n",
    "                ### Response:\n",
    "            \"\"\"\n",
    "        )\n",
    "        \n",
    "        formatted_prompt = prompt_template.format(context=context, user_query=query)\n",
    "        try:\n",
    "            \n",
    "            gemini_chat_model = ChatGoogleGenerativeAI(\n",
    "                model=\"gemini-1.5-flash\",\n",
    "                api_key=GEMINI_API_KEY,\n",
    "                temperature=0.3,\n",
    "            )\n",
    "            response = gemini_chat_model([HumanMessage(content=formatted_prompt)]) \n",
    "            answer = response.content\n",
    "        except Exception as e:\n",
    "            raise HTTPException(status_code=500,   detail=f\"Failed to generate response: {str(e)}\") \n",
    "        return answer\n",
    "    \n",
    "    except HTTPException as e:\n",
    "        raise e\n",
    "    except Exception as general_exc:\n",
    "        raise HTTPException( status_code=500,detail={f\"An unexpected error occurred,  {str(general_exc)}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DB_URI  = \"postgresql://postgres:mypassword@localhost:5432/temp_db\"    \n",
    "from psycopg_pool import ConnectionPool\n",
    "from langgraph.checkpoint.postgres import PostgresSaver\n",
    " \n",
    "connection_kwargs = {\"autocommit\": True, \"prepare_threshold\": 0}\n",
    " \n",
    "pool = ConnectionPool(conninfo=DB_URI, max_size=20, kwargs=connection_kwargs)\n",
    " \n",
    "checkpointer = PostgresSaver(pool)\n",
    "checkpointer.setup()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "def register_complaint(complaint_id: int, name: str, contact: str, complaint_details: str) -> str:\n",
    "    \"\"\"\n",
    "    Register a new complaint.\n",
    "    \n",
    "    Args: \n",
    "    complaint_id (int): A unique ID for the complaint.\n",
    "    name (str): The name of the complainant.\n",
    "    complaint_details (str): Details of the complaint.\n",
    "    \n",
    "    Returns:\n",
    "    str: Confirmation message with the complaint ID.\n",
    "    \"\"\"\n",
    "     \n",
    "    return f\"Complaint registered successfully! Your complaint ID is {complaint_id}.\"\n",
    "def check_complaint_status( complaint_id: int) -> str:\n",
    "    \"\"\"\n",
    "    Check the status of a complaint.\n",
    "    \n",
    "    Args:\n",
    "    complaint_id (int): The ID of the complaint to check.\n",
    "    \n",
    "    Returns:\n",
    "    str: The status of the complaint or a message if the complaint ID is not found.\n",
    "    \"\"\"\n",
    "    complaints_db = [\n",
    "    {\n",
    "        \"id\": 1,\n",
    "        \"name\": \"Arman\",\n",
    "        \"status\": \"pending\",\n",
    "        \"details\": \"Solve in the next day.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 2,\n",
    "        \"name\": \"Ayesha\",\n",
    "        \"status\": \"fulfilled\",\n",
    "        \"details\": \"Issue resolved on 2024-12-15.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 3,\n",
    "        \"name\": \"Rahim\",\n",
    "        \"status\": \"pending\",\n",
    "        \"details\": \"Will be addressed within 48 hours.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 4,\n",
    "        \"name\": \"Sara\",\n",
    "        \"status\": \"fulfilled\",\n",
    "        \"details\": \"Issue resolved on 2024-12-10.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": 5,\n",
    "        \"name\": \"Ali\",\n",
    "        \"status\": \"pending\",\n",
    "        \"details\": \"Awaiting confirmation from the user.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "    for complaint in complaints_db:\n",
    "        if complaint[\"id\"] == complaint_id:\n",
    "            return (\n",
    "                f\"Complaint ID: {complaint['id']}\\n\"\n",
    "                f\"Name: {complaint['name']}\\n\"\n",
    "                f\"Status: {complaint['status']}\\n\"\n",
    "                f\"Details: {complaint['details']}\"\n",
    "            )\n",
    "    return f\"No complaint found with ID {complaint_id}.\"\n",
    "\n",
    "def search_from_rag(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Perform a search from the RAG (Retrieval-Augmented Generation) system.\n",
    "    \n",
    "    This function determines whether the user's query is related to registering a complaint \n",
    "    or checking the status of a complaint. If the query is related to these actions, \n",
    "    the RAG system is not called. For all other queries, the function calls the `rag_system`\n",
    "    function and passes the query to search for relevant information.\n",
    "    Args:\n",
    "        query (str): The user query that will be processed.\n",
    "    \n",
    "    Returns:\n",
    "        str: The generated response after retrieving relevant data from the RAG.\n",
    "    \"\"\"\n",
    "    query_embedding = rag_system(query)\n",
    "    return query_embedding\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Key 'title' is not supported in schema, ignoring\n",
      "Key 'title' is not supported in schema, ignoring\n",
      "Key 'title' is not supported in schema, ignoring\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langgraph.graph import MessagesState\n",
    "class State(MessagesState):\n",
    "    summary: str\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "llm = ChatGoogleGenerativeAI(api_key = GEMINI_API_KEY,model='gemini-1.5-flash',temperature=0.2)\n",
    "llm_with_tools = llm.bind_tools([register_complaint,check_complaint_status, search_from_rag])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "\n",
    "\n",
    "def tool_calling_llm(state: MessagesState) -> MessagesState:\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "builder: StateGraph = StateGraph(MessagesState)\n",
    "builder.add_node(\"tool_calling_llm\", tool_calling_llm)\n",
    "builder.add_node(\"tools\", ToolNode([register_complaint,check_complaint_status,search_from_rag]))\n",
    "\n",
    "\n",
    "builder.add_node(\"summarize_node\", tool_calling_llm)\n",
    "builder.add_edge(START, \"tool_calling_llm\")\n",
    "builder.add_conditional_edges(\n",
    "    \"tool_calling_llm\", \n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", 'tool_calling_llm')\n",
    "\n",
    "builder.add_edge(\"summarize_node\", END)\n",
    "graph: CompiledStateGraph = builder.compile()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "hi\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Hello! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "messages = [HumanMessage(content=\"hi\")]\n",
    "messages = graph.invoke({\"messages\": messages})\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "What should I do if I receive a damaged or incorrect item?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "To report a damaged or incorrect item, you can register a complaint with our system.  I'll need some information from you to do so.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "messages = [HumanMessage(content=\"What should I do if I receive a damaged or incorrect item?\")]\n",
    "messages = graph.invoke({\"messages\": messages} )\n",
    "for m in messages['messages']:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

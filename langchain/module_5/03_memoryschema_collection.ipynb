{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot with Collection Schema \n",
    "\n",
    "## Review\n",
    "\n",
    "We extended our chatbot to save semantic memories to a single [user profile](https://langchain-ai.github.io/langgraph/concepts/memory/#profile). \n",
    "\n",
    "We also introduced a library, [Trustcall](https://github.com/hinthornw/trustcall), to update this schema with new information. \n",
    "\n",
    "## Goals\n",
    "\n",
    "Sometimes we want to save memories to a [collection](https://docs.google.com/presentation/d/181mvjlgsnxudQI6S3ritg9sooNyu4AcLLFH1UK0kIuk/edit#slide=id.g30eb3c8cf10_0_200) rather than single profile. \n",
    "\n",
    "Here we'll update our chatbot to [save memories to a collection](https://langchain-ai.github.io/langgraph/concepts/memory/#collection).\n",
    "\n",
    "We'll also show how to use [Trustcall](https://github.com/hinthornw/trustcall) to update this collection. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'NoneType'>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%%capture --no-stderr\n",
    "%pip install -U trustcall langchain_google_genai langgraph langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded from .env file.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not GEMINI_API_KEY:\n",
    "    raise ValueError(\"GEMINI_API_KEY is not set in the .env file.\")\n",
    "\n",
    "print(\"API key loaded from .env file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a collection schema\n",
    "\n",
    "Instead of storing user information in a fixed profile structure, we'll create a flexible collection schema to store memories about user interactions.\n",
    "\n",
    "Each memory will be stored as a separate entry with a single `content` field for the main information we want to remember\n",
    "\n",
    "This approach allows us to build an open-ended collection of memories that can grow and change as we learn more about the user.\n",
    "\n",
    "We can define a collection schema as a [Pydantic](https://docs.pydantic.dev/latest/) object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from pydantic import BaseModel,Field\n",
    "class Memory(BaseModel):\n",
    "    content:str = Field(description='The main content of the memory. For example:  likeness,  learning interests ...')\n",
    "class MemoryCollection(BaseModel): \n",
    "    final_memory: List[Memory] = Field(description='A list of memories about the user.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can used LangChain's chat model [chat model](https://python.langchain.com/docs/concepts/chat_models/) interface's [`with_structured_output`](https://python.langchain.com/docs/concepts/structured_outputs/#recommended-usage) method to enforce structured output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"My name is arman , I want to learn AI and i like programming .I am from pakistan. I am 23 year's old\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-flash',api_key=GEMINI_API_KEY)\n",
    "model_with_structure = model.with_structured_output(Memory)\n",
    "structured_output = model_with_structure.invoke([HumanMessage(\"My name is arman , I want to learn AI and i like programming .I am from pakistan. I am 23 year's old\")])\n",
    "structured_output.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from typing import TypedDict, List\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    content:str = Field(description='The main content of the memory. For example:  likeness,  learning interests ...')\n",
    "class MemoryCollection(BaseModel): \n",
    "    final_memory: List[Memory] = Field(description='A list of memories about the user.')\n",
    "\n",
    "model = ChatGoogleGenerativeAI(model='gemini-1.5-flash',api_key=GEMINI_API_KEY)\n",
    "model_with_structure = model.with_structured_output(MemoryCollection)\n",
    "structured_output = model_with_structure.invoke([HumanMessage(\"My name is arman , I want to learn AI and i like programming . I am from pakistan. I am 23 year's old\")])\n",
    "structured_output.final_memory[0].content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `model_dump()` to serialize a Pydantic model instance into a Python dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structured_output.content[0].model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating collection schema\n",
    "\n",
    "We discussed the challenges with updating a profile schema in the last lesson. \n",
    "\n",
    "The same applies for collections! \n",
    "\n",
    "We want the ability to update the collection with new memories as well as update existing memories in the collection. \n",
    "\n",
    "Now we'll show that [Trustcall](https://github.com/hinthornw/trustcall) can be also used to update a collection. \n",
    "\n",
    "This enables both addition of new memories as well as [updating existing memories in the collection](https://github.com/hinthornw/trustcall?tab=readme-ov-file#simultanous-updates--insertions\n",
    ").\n",
    "\n",
    "Let's define a new extractor with Trustcall. \n",
    "\n",
    "As before, we provide the schema for each memory, `Memory`.  \n",
    "\n",
    "But, we can supply `enable_inserts=True` to allow the extractor to insert new memories to the collection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trustcall import create_extractor\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "from typing import TypedDict, List\n",
    "from pydantic import BaseModel,Field\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    content:str = Field(description='The main content of the memory. For example:  likeness,  learning interests ...')\n",
    "\n",
    "trust_call = create_extractor(model,tools=[Memory],tool_choice='Memory')\n",
    "\n",
    "instruction = \"\"\"Extract memories from the following conversation:\"\"\"\n",
    "\n",
    "  \n",
    "chatting = [HumanMessage(content=\"Hi, I'm arman.\"), \n",
    "                AIMessage(content=\"Nice to meet you, arman.\"), \n",
    "                HumanMessage(content=\", I want to learn AI and i like programming . I am from pakistan. I am 23 year's old\")]\n",
    "final_msg = {\"messages\": [SystemMessage(content=instruction)] + chatting}\n",
    "\n",
    "resp = trust_call.invoke(final_msg)\n",
    " \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_memories=[(str(i),\"Memory\",memory.model_dump()) for i, memory in enumerate(resp[\"responses\"])] if resp[\"responses\"] else None\n",
    "print(existing_memories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_conversation = [AIMessage(content=\"Today programming is a broad field but AI can replace the jobs of small level project\"),\n",
    "                        HumanMessage(content=\"I want to go in this summar holidays in islambad\"),]\n",
    " \n",
    "sys_msg = \"\"\"Update existing memories and create new ones based on the following conversation:\"\"\"\n",
    "final_msg = {\"messages\": [SystemMessage(content=sys_msg)] + new_conversation,'existing':existing_memories}\n",
    "new_resp = trust_call.invoke(final_msg)\n",
    "print(new_resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in new_resp[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot with collection schema updating\n",
    "\n",
    "Now, let's bring Trustcall into our chatbot to create and update a memory collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.store.base import BaseStore\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from pydantic import BaseModel, Field \n",
    "from typing import List\n",
    "import uuid\n",
    "from langchain_core.messages import merge_message_runs\n",
    "\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory. For example: User expressed interest in learning about French.\")\n",
    "\n",
    "MODEL_SYSTEM_MESSAGE = \"\"\"You are a helpful chatbot. You are designed to be a companion to a user. \n",
    "\n",
    "You have a long term memory which keeps track of information you learn about the user over time.\n",
    "\n",
    "Current Memory (may include updated memories from this conversation): \n",
    "\n",
    "{memory}\"\"\"\n",
    "\n",
    "trustcall_extractor = create_extractor(\n",
    "    model,\n",
    "    tools=[Memory],\n",
    "    tool_choice=\"Memory\", \n",
    ")\n",
    "TRUSTCALL_INSTRUCTION = \"\"\"Reflect on following interaction. \n",
    "\n",
    "Use the provided tools to retain any necessary memories about the user. \n",
    "\n",
    "Use parallel tool calling to handle updates and insertions simultaneously:\"\"\"\n",
    "\n",
    "def call_model(state:MessagesState,config:RunnableConfig,store:BaseStore):\n",
    "  user_id = config[\"configurable\"][\"user_id\"]\n",
    "  namespace = (\"memory\", user_id) \n",
    "\n",
    "  existing_memory = store.search(namespace) \n",
    "  formatted_memory = \"\\n\".join(f\"- {mem.value['content']}\" for mem in existing_memory)\n",
    " \n",
    "  sys_msg = MODEL_SYSTEM_MESSAGE.format(memory=formatted_memory)\n",
    " \n",
    "  messages = [SystemMessage(content=sys_msg)] + state[\"messages\"]\n",
    "\n",
    "  resp = model.invoke(messages)\n",
    "  return {\"messages\":resp}\n",
    "\n",
    "\n",
    "\n",
    "CREATE_MEMORY_INSTRUCTION = \"\"\"\"You are collecting information about\n",
    "the user to personalize your responses.\n",
    "\n",
    "CURRENT USER INFORMATION:\n",
    "{memory}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "1. Review the chat history below carefully\n",
    "2. Identify new information about the user, such as:\n",
    "   - Personal details (name, location)\n",
    "   - Preferences (likes, dislikes)\n",
    "   - Interests and hobbies\n",
    "   - Past experiences\n",
    "   - Goals or future plans\n",
    "3. Merge any new information with existing memory\n",
    "4. Format the memory as a clear, bulleted list\n",
    "5. If new information conflicts with existing memory, keep the most recent version\n",
    "\n",
    "Remember: Only include factual information directly stated by the user. Do not make assumptions or inferences.\n",
    "\n",
    "Based on the chat history below, please update the user information:\"\"\"\n",
    "def call_summary(state:MessagesState,config:RunnableConfig,store:BaseStore):\n",
    "  user_id = config[\"configurable\"][\"user_id\"]\n",
    "  namespace = (\"memory\", user_id) \n",
    "  existing_items = store.search(namespace) \n",
    "  print(existing_items)\n",
    "  print(f'typeof {type(existing_items)}')\n",
    "\n",
    "   \n",
    "  existing_memories = [(existing_item.key, 'Memory', existing_item.value) for existing_item in existing_items] if existing_items else None\n",
    "\n",
    "  updated_messages=list(merge_message_runs(messages=[SystemMessage(content=TRUSTCALL_INSTRUCTION)] + state[\"messages\"]))\n",
    " \n",
    "  result = trustcall_extractor.invoke({\"messages\": updated_messages, \n",
    "                                      \"existing\": existing_memories}) \n",
    "  for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "      store.put(namespace,rmeta.get(\"json_doc_id\", str(uuid.uuid4())), r.model_dump(mode=\"json\"), )\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "graph = StateGraph(MessagesState)\n",
    "graph.add_node('call_model',call_model)\n",
    "graph.add_node('call_summary',call_summary)\n",
    "\n",
    "graph.add_edge(START,'call_model') \n",
    "graph.add_edge('call_model','call_summary')\n",
    "graph.add_edge('call_summary',END)\n",
    "\n",
    "short_term_memory = MemorySaver()\n",
    "long_term_memory = InMemoryStore()\n",
    "compiled_graph = graph.compile(checkpointer=short_term_memory,store=long_term_memory) \n",
    "\n",
    "display(Image(compiled_graph.get_graph().draw_mermaid_png()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}} \n",
    "input_messages = [HumanMessage(content=\"Hi, my name is Arman, and I am from Sialkot. I am 23 years old.\")] \n",
    "for chunk in compiled_graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}} \n",
    "input_messages = [HumanMessage(content=\"I like programming\")] \n",
    "for chunk in compiled_graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"1\"\n",
    "namespace = (\"memory\", user_id) \n",
    "memories = long_term_memory.search(namespace)\n",
    "for m in memories:\n",
    "    print(m.dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"I want to buy shoes and a dress.\")] \n",
    "for chunk in compiled_graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_messages = [HumanMessage(content=\"I'm looking for black sneakers with a budget of $300 and a casual dress in white, with a budget of $400 and a waist size of 36 \")] \n",
    "for chunk in compiled_graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\", \"user_id\": \"1\"}} \n",
    "input_messages = [HumanMessage(content=\"what do you know about me\")] \n",
    "for chunk in compiled_graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue the conversation in a new thread."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"2\", \"user_id\": \"1\"}} \n",
    "input_messages = [HumanMessage(content=\"what do you know about me\")] \n",
    "for chunk in compiled_graph.stream({\"messages\": input_messages}, config, stream_mode=\"values\"):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import (\n",
    "    merge_message_runs,\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolCall,\n",
    ")\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(\"you're a good assistant.\"),\n",
    "    HumanMessage(\"what's your favorite color\", id=\"foo\",),\n",
    "    HumanMessage(\"wait your favorite food\", id=\"bar\",),\n",
    "    AIMessage(\n",
    "        \"my favorite colo\",\n",
    "        tool_calls=[ToolCall(name=\"blah_tool\", args={\"x\": 2}, id=\"123\", type=\"tool_call\")],\n",
    "        id=\"baz\",\n",
    "    ),\n",
    "    AIMessage(\n",
    "        [{\"type\": \"text\", \"text\": \"my favorite dish is lasagna\"}],\n",
    "        tool_calls=[ToolCall(name=\"blah_tool\", args={\"x\": -10}, id=\"456\", type=\"tool_call\")],\n",
    "        id=\"blur\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "merge_message_runs(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LangSmith \n",
    "\n",
    "https://smith.langchain.com/public/c87543ec-b426-4a82-a3ab-94d01c01d9f4/r\n",
    "\n",
    "## Studio\n",
    "\n",
    "![Screenshot 2024-10-30 at 11.29.25 AM.png](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/6732d0876d3daa19fef993ba_Screenshot%202024-11-11%20at%207.50.21%E2%80%AFPM.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr \n",
    "!pip install -q langgraph langchain python-dotenv  langsmith langchain_google_genai langchain_core langchain_community tavily-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded from .env file.\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os \n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "if not api_key:\n",
    "    raise ValueError(\"GEMINI_API_KEY is not set in the .env file.\")\n",
    "\n",
    "print(\"API key loaded from .env file.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings,ChatGoogleGenerativeAI \n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "faq_data = [\n",
    "    {'question': \"How do I track my order?\", 'answer': \"You can track your order by logging into your account and going to the 'Order History' section.\"},\n",
    "    {'question': \"What is your return policy?\", 'answer': \"Our return policy allows returns within 30 days of purchase.\"},\n",
    "    {\"question\": \"Can I cancel or modify my order after placing it?\",\"answer\": \"Yes, you can cancel or modify your order within 24 hours of placing it by contacting our customer support team.\"},\n",
    "    {\"question\": \"What should I do if I receive a damaged or incorrect item?\", \"answer\": \"If you receive a damaged or incorrect item, please contact our support team immediately with your order details and a photo of the item.\"},\n",
    "    {\"question\": \"Do you offer discounts or promotional codes?\",\"answer\": \"Yes, we regularly offer discounts and promotional codes. Sign up for our newsletter or follow us on social media to stay updated.\"},\n",
    "    {\"question\": \"How long does delivery take?\",  \"answer\": \"Delivery typically takes 3-7 business days for domestic orders and 7-14 business days for international orders.\"},\n",
    "    {\"question\": \"Is my personal information secure?\", \"answer\": \"Yes, we use industry-standard encryption and security measures to protect your personal information and ensure safe transactions.\"}\n",
    "]\n",
    "\n",
    "\n",
    "gemini_embeddings = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\" ,google_api_key=GEMINI_API_KEY)\n",
    "def faq_data_with_embeddings(faq_data):\n",
    "    for faq in faq_data:\n",
    "        faq['embedding'] = gemini_embeddings.embed_query(faq['question'])\n",
    "    return faq_data\n",
    "\n",
    "\n",
    "# enriched_faq_data = faq_data_with_embeddings(faq_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"i want to track my order and what is my name\"\n",
    "query_embedding = gemini_embeddings.embed_query(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import List\n",
    "\n",
    "def find_similar(vectorA:List[int],vectorB:List[int]) -> int :\n",
    "    cosine_sim = np.dot(vectorA, vectorB) / (np.linalg.norm(vectorA) * np.linalg.norm(vectorB))    \n",
    "    return cosine_sim \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index , faq in enumerate(enriched_faq_data):\n",
    "    sim = find_similar(query_embedding,faq[\"embedding\"])\n",
    "    enriched_faq_data[index][\"cosine_similarity\"] = sim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCHING_THRESHOLD = 8\n",
    "filtered_faq = [(faq[\"question\"],faq[\"answer\"]) for faq in enriched_faq_data if faq[\"cosine_similarity\"] >= MATCHING_THRESHOLD]\n",
    "for question,answer in filtered_faq:\n",
    "    print(f\"Question:{question},Answer:{answer}\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: You can track your order by logging into your account and going to the 'Order History' section.  I’m sorry, I don’t know the answer to that right now. (regarding your name).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "context = \"\\n\".join([f\"Question: {faq['question']}\\nAnswer: {faq['answer']}\" for faq in faq_data])\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"user_query\"],\n",
    "    template=\"\"\"\n",
    "        You are a helpful and knowledgeable chatbot for an e-commerce website. Your job is to answer customer questions based on the provided context. If the answer cannot be found in the context, respond politely with: \"I’m sorry, I don’t know the answer to that right now.\"\n",
    "\n",
    "        ### Context:  \n",
    "        {context}\n",
    "\n",
    "        ### User Question:  \n",
    "        {user_query}\n",
    "\n",
    "        ### Guidelines for Responses:\n",
    "        1. Answer based on the provided context.\n",
    "        2. Be concise, friendly, and professional.\n",
    "        3. If unsure, say: \"I’m sorry, I don’t know the answer to that right now.\"\n",
    "        4. Offer help where possible, like checking stock, suggesting products, or explaining policies.\n",
    "\n",
    "        ### Response:\n",
    "    \"\"\"\n",
    ") \n",
    "formatted_prompt = prompt_template.format(context=context, user_query=user_query)\n",
    "gemini_chat_model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    api_key= GEMINI_API_KEY,\n",
    "    temperature=0.3,\n",
    ")\n",
    "response = gemini_chat_model([HumanMessage(content=formatted_prompt)]) \n",
    "print(\"Answer:\", response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

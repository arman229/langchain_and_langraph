{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31merror\u001b[39m                                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 118\u001b[39m\n\u001b[32m    107\u001b[39m scene_data = [\n\u001b[32m    108\u001b[39m     {\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33m1\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    109\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mScene\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mSunny Meadow\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    114\u001b[39m      \u001b[33m'\u001b[39m\u001b[33mSave_image_path\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33marman_output\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mNew_Generated_images\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mimages_20250403064306\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[33mimage_scene_1.png\u001b[39m\u001b[33m'\u001b[39m},\n\u001b[32m    115\u001b[39m ]\n\u001b[32m    117\u001b[39m output_filename = \u001b[33m\"\u001b[39m\u001b[33mtypewriter_animatiosssssssssn.mp4\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m118\u001b[39m \u001b[43mcreate_video_with_typewriter_effect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscene_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_filename\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mcreate_video_with_typewriter_effect\u001b[39m\u001b[34m(scene_data, output_filename, fontsize, video_fps)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m image_path:\n\u001b[32m     62\u001b[39m     image = cv2.imread(image_path)\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m     image = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mvideo_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_height\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     64\u001b[39m     frames = zoom_in(image, num_frames) \u001b[38;5;28;01mif\u001b[39;00m idx % \u001b[32m2\u001b[39m == \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m zoom_out(image, num_frames)\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31merror\u001b[39m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:4152: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def split_text_into_segments(text, font, max_width):\n",
    "    words = text.split()\n",
    "    segments = []\n",
    "    current_segment = \"\"\n",
    "    for word in words:\n",
    "        test_line = current_segment + (\" \" if current_segment else \"\") + word\n",
    "        w = font.getbbox(test_line)[2]\n",
    "        if w <= max_width:\n",
    "            current_segment = test_line\n",
    "        else:\n",
    "            if current_segment:\n",
    "                segments.append(current_segment)\n",
    "            current_segment = word\n",
    "    if current_segment:\n",
    "        segments.append(current_segment)\n",
    "    print(f\"print segments: {segments}\")    \n",
    "    return segments\n",
    "\n",
    "def zoom_in(image, num_frames=30, zoom_factor=0.5):\n",
    "    frames = []\n",
    "    h, w = image.shape[:2]\n",
    "    for i in range(num_frames):\n",
    "        scale = 1.0 + (i / num_frames) * zoom_factor\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, 0, scale)\n",
    "        frame = cv2.warpAffine(image, M, (w, h))\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "def zoom_out(image, num_frames=30, zoom_factor=0.5):\n",
    "    frames = []\n",
    "    h, w = image.shape[:2]\n",
    "    for i in range(num_frames):\n",
    "        scale = 1.0 + ((num_frames - i - 1) / num_frames) * zoom_factor\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, 0, scale)\n",
    "        frame = cv2.warpAffine(image, M, (w, h))\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "def create_video_with_typewriter_effect(scene_data, output_filename, fontsize=60, video_fps=15):\n",
    "    font = ImageFont.truetype(\"arial.ttf\", fontsize)\n",
    "    video_width = 1080\n",
    "    video_height = 1920\n",
    "    video_size = (video_width, video_height)\n",
    "    video = cv2.VideoWriter(output_filename, \n",
    "                            cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "                            video_fps, \n",
    "                            video_size)\n",
    "    \n",
    "    for idx, scene in enumerate(scene_data):\n",
    "        narration = scene['Narration']\n",
    "        scene_duration = scene['Audio_duration']\n",
    "        image_path = scene.get('Save_image_path', None)\n",
    "        num_frames = int(scene_duration * video_fps)\n",
    "        \n",
    "        if image_path:\n",
    "            image = cv2.imread(image_path)\n",
    "            image = cv2.resize(image, (video_width, video_height))\n",
    "            frames = zoom_in(image, num_frames) if idx % 2 == 0 else zoom_out(image, num_frames)\n",
    "        else:\n",
    "            frames = [np.zeros((video_height, video_width, 3), dtype=np.uint8) for _ in range(num_frames)]\n",
    "        \n",
    "        segments = split_text_into_segments(narration, font, video_width - 40)\n",
    "        num_segments = len(segments)\n",
    "        if num_segments == 0:\n",
    "            continue\n",
    "        \n",
    "        total_chars = sum(len(segment) for segment in segments)\n",
    "        \n",
    "        # Allocate time to each segment proportionally to its length\n",
    "        for segment in segments:\n",
    "            seg_chars = len(segment)\n",
    "            segment_duration = (seg_chars / total_chars) * scene_duration\n",
    "            segment_frames = int(segment_duration * video_fps)\n",
    "            total_chars_in_segment = len(segment)\n",
    "            for frame_idx in range(segment_frames):\n",
    "                frame = frames[frame_idx % num_frames].copy()\n",
    "                frame_pil = Image.fromarray(frame)\n",
    "                draw = ImageDraw.Draw(frame_pil)\n",
    "                \n",
    "                # Ensure final frame shows full text\n",
    "                char_count = int(((frame_idx + 1) / segment_frames) * total_chars_in_segment)\n",
    "                displayed_text = segment[:char_count]\n",
    "                \n",
    "                text_width, text_height = font.getbbox(displayed_text)[2:4]\n",
    "                x = (video_width - text_width) // 2\n",
    "                y = (video_height - text_height) // 2\n",
    "                \n",
    "                # Draw a black rectangle as background behind the text\n",
    "                padding = 10\n",
    "                rect_coords = [(x - padding, y - padding), (x + text_width + padding, y + text_height + padding)]\n",
    "                draw.rectangle(rect_coords, fill=(0, 0, 0))\n",
    "                \n",
    "                # Now draw the text on top\n",
    "                draw.text((x, y), displayed_text, font=font, fill=(255, 255, 255))\n",
    "                \n",
    "                video.write(np.array(frame_pil))\n",
    "    \n",
    "    video.release()\n",
    "    print(f\"Video saved as {output_filename}\")\n",
    "\n",
    "scene_data = [\n",
    "    {'id': '1',\n",
    "     'Scene': 'Sunny Meadow',\n",
    "     'Description': 'Sunny Meadow where Barnaby and Sheldon meet near the big oak tree.',\n",
    "     'Narration': 'One sunny morning, Barnaby hopped past Sheldon Shelldon, a tortoise whose shell was a beautiful shade of deep green. Sheldon was slowly, slowly making his way to the big oak tree   ',\n",
    "     'Save_audio_path': 'arman_output\\\\New_Generated_voices\\\\voices_20250403064254\\\\voice_scene_1.mp3',\n",
    "     'Audio_duration': 12.55,\n",
    "     'Save_image_path': 'arman_output\\\\New_Generated_images\\\\images_20250403064306\\\\image_scene_1.png'},\n",
    "]\n",
    "\n",
    "output_filename = \"typewriter_animatiosssssssssn.mp4\"\n",
    "create_video_with_typewriter_effect(scene_data, output_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############# Static caption ###########3\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from moviepy import *\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import json\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "import json\n",
    "from typing import Sequence\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage\n",
    "from typing import Annotated\n",
    "from IPython.display import display, Markdown\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "# ----------------------- Data Types -----------------------\n",
    "class ClassObject(TypedDict):\n",
    "    Object: str\n",
    "    Description: str\n",
    "\n",
    "class MainCharacters(TypedDict):\n",
    "    Name: str\n",
    "    Appearance: str\n",
    "    Characteristics: str\n",
    "\n",
    "class SupportingCharacters(TypedDict):\n",
    "    Name: str\n",
    "    Appearance: str\n",
    "    Characteristics: str\n",
    "\n",
    "class ScenesList(TypedDict):\n",
    "    id: str\n",
    "    Scene: str\n",
    "    Description: str\n",
    "    Narration: str\n",
    "    Img_prompt: str\n",
    "    Save_audio_path: str\n",
    "    Save_image_path: str\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    MainCharacters: list[MainCharacters]\n",
    "    SupportingCharacters: list[SupportingCharacters]\n",
    "    Scene_list: list[ScenesList]\n",
    "    Objects: list[ClassObject]\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    pre_processing_video_path: str\n",
    "    Voices_folder: str\n",
    "    Images_folder: str\n",
    "\n",
    "class SubState(TypedDict):\n",
    "    current_scene: ScenesList\n",
    "    output_folder: str\n",
    "\n",
    "# ----------------------- Utility Functions -----------------------\n",
    "def split_text_into_segments(text, font, max_width):\n",
    "    \"\"\"Split the text into segments that fit within max_width.\"\"\"\n",
    "    words = text.split()\n",
    "    segments = []\n",
    "    current_segment = \"\"\n",
    "    for word in words:\n",
    "        test_line = current_segment + (\" \" if current_segment else \"\") + word\n",
    "        w = font.getbbox(test_line)[2]\n",
    "        if w <= max_width:\n",
    "            current_segment = test_line\n",
    "        else:\n",
    "            if current_segment:\n",
    "                segments.append(current_segment)\n",
    "            current_segment = word\n",
    "    if current_segment:\n",
    "        segments.append(current_segment)\n",
    "    print(f\"print segments: {segments}\")    \n",
    "    return segments\n",
    "\n",
    "def zoom_in(image, num_frames=30, zoom_factor=0.5):\n",
    "    \"\"\"Generates frames for a zoom-in effect.\"\"\"\n",
    "    frames = []\n",
    "    h, w = image.shape[:2]\n",
    "    for i in range(num_frames):\n",
    "        scale = 1.0 + (i / num_frames) * zoom_factor\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, 0, scale)\n",
    "        frame = cv2.warpAffine(image, M, (w, h))\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "def zoom_out(image, num_frames=30, zoom_factor=0.5):\n",
    "    \"\"\"Generates frames for a zoom-out effect without blinking.\"\"\"\n",
    "    frames = []\n",
    "    h, w = image.shape[:2]\n",
    "    for i in range(num_frames):\n",
    "        scale = 1.0 + ((num_frames - i - 1) / num_frames) * zoom_factor\n",
    "        center = (w // 2, h // 2)\n",
    "        M = cv2.getRotationMatrix2D(center, 0, scale)\n",
    "        frame = cv2.warpAffine(image, M, (w, h))\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "def fade_in(image, num_frames=2):\n",
    "    \"\"\"Creates a fade-in effect from black to the image.\"\"\"\n",
    "    frames = []\n",
    "    black = np.zeros_like(image)\n",
    "    for i in range(num_frames):\n",
    "        alpha = i / num_frames\n",
    "        frame = cv2.addWeighted(image, alpha, black, 1 - alpha, 0)\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "def fade_out(image, num_frames=30):\n",
    "    \"\"\"Creates a fade-out effect from image to black.\"\"\"\n",
    "    frames = []\n",
    "    black = np.zeros_like(image)\n",
    "    for i in range(num_frames):\n",
    "        alpha = 1 - i / num_frames\n",
    "        frame = cv2.addWeighted(image, alpha, black, 1 - alpha, 0)\n",
    "        frames.append(frame)\n",
    "    return frames\n",
    "\n",
    "# ----------------------- Pre-processing Video Function -----------------------\n",
    "def pre_processing_video(state: GraphState):\n",
    "    fps = 2\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    dynamic_folder = f\"pre_video_{timestamp}\"\n",
    "    output_folder = os.path.join(\"Pre_Generated_videos\", dynamic_folder)\n",
    "    output_folder = os.path.join(\"output\", output_folder)\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    file_name = f'pre_video_{timestamp}.mp4'\n",
    "    \n",
    "    pre_processing_video_path = os.path.join(output_folder, file_name)\n",
    "    scene_list = state['Scene_list']\n",
    "    if not scene_list:\n",
    "        raise ValueError(\"No scenes provided.\")\n",
    "\n",
    "    first_img = cv2.imread(scene_list[0][\"Save_image_path\"])\n",
    "    if first_img is None:\n",
    "        raise ValueError(f\"Cannot load image: {scene_list[0]['Save_image_path']}\")\n",
    "    \n",
    "    h, w, _ = first_img.shape\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(pre_processing_video_path, fourcc, fps, (w, h))\n",
    "    \n",
    "    # Load a font for captions (adjust size as needed)\n",
    "    caption_font = ImageFont.truetype(\"arial.ttf\", 32)\n",
    "    # Maximum width for caption text (with some margin)\n",
    "    max_caption_width = w - 40\n",
    "    \n",
    "    for idx, scene in enumerate(scene_list):\n",
    "        img_path = scene[\"Save_image_path\"]\n",
    "        duration = float(scene[\"Audio_duration\"])\n",
    "        narration = scene[\"Narration\"]\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Cannot load image {img_path}. Skipping.\")\n",
    "            continue\n",
    "        img = cv2.resize(img, (w, h))\n",
    "        \n",
    "        total_frames = int(fps * duration)\n",
    "        fade_in_frame = int(fps * 1)\n",
    "        fade_out_frame = int(fps * 1)\n",
    "        first_image_duration = total_frames - fade_out_frame\n",
    "        \n",
    "        # Generate zoom/fade effect frames\n",
    "        if idx == 0:\n",
    "            zoom_in_frames = zoom_in(img, num_frames=first_image_duration, zoom_factor=0.5)\n",
    "            final_zoom_frame = zoom_in_frames[-1]\n",
    "            fade_out_frames = fade_out(final_zoom_frame, num_frames=fade_out_frame)\n",
    "            effect_frames = zoom_in_frames + fade_out_frames\n",
    "        elif idx % 2 == 1:\n",
    "            zoom_out_frames = zoom_out(img, num_frames=first_image_duration, zoom_factor=0.5)\n",
    "            final_zoom_frame = zoom_out_frames[-1]\n",
    "            fade_out_frames = fade_out(final_zoom_frame, num_frames=fade_out_frame)\n",
    "            effect_frames = zoom_out_frames + fade_out_frames\n",
    "        else:\n",
    "            fade_in_frames = fade_in(img, num_frames=fade_in_frame)\n",
    "            zoom_in_frames = zoom_in(img, num_frames=first_image_duration, zoom_factor=0.5)\n",
    "            final_zoom_frame = zoom_in_frames[-1]\n",
    "            fade_out_frames = fade_out(final_zoom_frame, num_frames=fade_out_frame)\n",
    "            effect_frames = fade_in_frames + zoom_in_frames + fade_out_frames\n",
    "        \n",
    "        # Use the split_text_into_segments logic to divide the narration into caption segments\n",
    "        segments = split_text_into_segments(narration, caption_font, max_caption_width)\n",
    "        num_segments = len(segments)\n",
    "        if num_segments == 0:\n",
    "            current_caption = \"\"\n",
    "        else:\n",
    "            # Determine frames per caption segment (display each segment for a proportionate time)\n",
    "            frames_per_segment = total_frames // num_segments\n",
    "        \n",
    "        # Overlay the caption on each frame\n",
    "        for frame_idx in range(total_frames):\n",
    "            frame = effect_frames[frame_idx].copy()\n",
    "            frame_pil = Image.fromarray(frame)\n",
    "            draw = ImageDraw.Draw(frame_pil)\n",
    "            \n",
    "            if num_segments > 0:\n",
    "                # Determine current segment index based on frame index\n",
    "                segment_index = min(frame_idx // frames_per_segment, num_segments - 1)\n",
    "                current_caption = segments[segment_index]\n",
    "            else:\n",
    "                current_caption = \"\"\n",
    "            \n",
    "            # Measure text size and calculate position (bottom center with padding)\n",
    "            text_width, text_height = caption_font.getbbox(current_caption)[2:4]\n",
    "            x = (w - text_width) // 2\n",
    "            y = h - text_height - 50  # 50 pixels margin from bottom\n",
    "            \n",
    "            # Draw a black rectangle as a background for the caption text\n",
    "            padding = 10\n",
    "            draw.rectangle([(x - padding, y - padding), (x + text_width + padding, y + text_height + padding)], fill=(0, 0, 0))\n",
    "            # Draw the caption text in white\n",
    "            draw.text((x, y), current_caption, font=caption_font, fill=(255, 255, 255))\n",
    "            \n",
    "            video_writer.write(np.array(frame_pil))\n",
    "    \n",
    "    video_writer.release()\n",
    "    print(f\"pre_processing_video saved as {pre_processing_video_path}\") \n",
    "    state['pre_processing_video_path'] = pre_processing_video_path \n",
    "    return state\n",
    "\n",
    "# ----------------------- Workflow -----------------------\n",
    "workflow = StateGraph(GraphState)\n",
    "workflow.add_node('pre_processing_video', pre_processing_video)\n",
    "workflow.add_edge(START, 'pre_processing_video')\n",
    "workflow.add_edge('pre_processing_video', END)\n",
    "app = workflow.compile()\n",
    "\n",
    "# Invoke the workflow with your scene data (assumed to be available in resp3['Scene_list'])\n",
    "resp4 = app.invoke({\"Scene_list\": resp3['Scene_list']})\n",
    "print(resp4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
